import os
import random
import warnings
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from easydict import EasyDict as edict
with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=DeprecationWarning)
    from torch.utils.tensorboard import SummaryWriter
import torch
from mbpo import mbpo
os.environ['SDL_VIDEODRIVER'] = 'dummy'
os.environ['WANDB_NOTEBOOK_NAME'] = 'mbpo.py'

warnings.filterwarnings("ignore", category=DeprecationWarning)

plt.rcParams['figure.dpi'] = 100
device = torch.device("cpu")
exp = edict()

exp.exp_name = 'mbpo'  # algorithm name, in this case it should be 'DQN'
exp.env_id = "InvertedPendulum-v5" # 'HalfCheetah-v5'name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0
exp.device = device.type  # save the device type used to load tensors and perform tensor operations
exp.max_episode_steps = 1000
exp.set_random_seed = True  # set random seed for reproducibility of python, numpy and torch
exp.seed = 2

# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)
# if the project does not exist in wandb, it will be created automatically
wandb_prj_name = f"RL_{exp.env_id}"

# name prefix of output files generated by the notebook
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"

if exp.set_random_seed:
    random.seed(exp.seed)
    np.random.seed(exp.seed)
    torch.manual_seed(exp.seed)
    torch.backends.cudnn.deterministic = exp.set_random_seed

# initialize the parameters
hypp = edict()

# flags for logging purposes
exp.enable_wandb_logging = False
exp.capture_video = True

# flags to generate agent's average performance during training
exp.eval_agent = True  # disable to speed up training
exp.eval_count = 20
exp.eval_frequency = 500
exp.exp_type = None
# agent training specific parameters and hyperparameters
hypp.total_timesteps = 50000  # the training duration in number of time steps
hypp.gamma = 0.99  # decay factor of future rewards
hypp.tau = 1e-3
#tweak in learning rates according to the learning rate according to the need
#can I automate the learning rates???
#should work >>
#or automate runs in all
hypp.hidden_layers_actor =3
hypp.hidden_layers_critic = 3
hypp.learning_rate = 1e-4
hypp.learning_rate_actor = 3e-3
hypp.learning_rate_critic = 3e-3
hypp.learning_rate_model = 3e-3
hypp.real_img_ratio = 0.05
hypp.display_evaluation = True #display video evaluation
hypp.plot_training = True # plot training
hypp.update_param_frequency = 40
hypp.model_train_frequency = 5
hypp.rollout_schedule= [1, 15, 1, 1]


hypp.buffer_size = 40000  # the size of the replay memory buffer
hypp.kstep = 1
hypp.num_rollouts = 400
hypp.num_ensembles = 7
hypp.hidden_dim = 200 
hypp.batch_size = 256 
hypp.start_learning = 5000
hypp.train_frequency = 1
hypp.rollout_start = 500
hypp.max_ent_coef = 1.0
# reinit run_name 
exp.run_name = f"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}"
ddp = mbpo(exp,hypp)
ddp.train()